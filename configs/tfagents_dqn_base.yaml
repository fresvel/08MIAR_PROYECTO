env_name: Acrobot-v1
seed: 123

network:
  type: q  # q | dueling
  fc_layer_params: [128, 128]

policy:
  epsilon_greedy: 0.1

training:
  num_iterations: 200000
  initial_collect_steps: 1000
  collect_steps_per_iteration: 1
  batch_size: 64
  replay_buffer_capacity: 100000
  gamma: 0.99
  learning_rate: 0.001
  target_update_tau: 1.0
  target_update_period: 1000
  log_interval: 1000
  eval_interval: 10000
  num_eval_episodes: 5
  checkpoint_interval: 10000

variants:
  double_dqn: false

logging:
  output_dir: outputs
