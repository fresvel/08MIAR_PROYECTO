{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporte TF-Agents (Acrobot)\n",
    "\n",
    "Este notebook reporta resultados de entrenamientos realizados con el módulo `acrobot_tfagents`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import sys\n",
    "ROOT = Path('.').resolve()\n",
    "sys.path.append(str(ROOT / 'src'))\n",
    "\n",
    "from acrobot_dqn.plots import plot_training_logs, plot_eval_results\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificador del run a reportar\n",
    "RUN_ID = 'tf_base_01'\n",
    "\n",
    "RUN_DIR = Path('outputs') / 'runs' / RUN_ID\n",
    "CONFIG_PATH = RUN_DIR / 'config.yaml'\n",
    "EVAL_PATH = RUN_DIR / 'metrics' / 'eval.json'\n",
    "FIG_DIR = RUN_DIR / 'figures'\n",
    "LOG_PATH = RUN_DIR / 'logs' / 'train_log.json'\n",
    "\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f'No se encontro {CONFIG_PATH}.')\n",
    "\n",
    "cfg = yaml.safe_load(CONFIG_PATH.read_text(encoding='utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar hiperparámetros del run\n",
    "config_df = pd.json_normalize(cfg, sep='.')\n",
    "config_df = config_df.T.rename(columns={0: 'value'})\n",
    "display(config_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficas de entrenamiento\n",
    "if LOG_PATH.exists():\n",
    "    plot_training_logs(LOG_PATH, FIG_DIR)\n",
    "    for name in ['training_reward.png', 'training_mean_q.png']:\n",
    "        img_path = FIG_DIR / name\n",
    "        if img_path.exists():\n",
    "            display(Image(filename=str(img_path)))\n",
    "else:\n",
    "    print(f'No se encontro el log: {LOG_PATH}')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados de evaluación\n",
    "if EVAL_PATH.exists():\n",
    "    results = json.loads(EVAL_PATH.read_text(encoding='utf-8'))\n",
    "    display(pd.DataFrame([{\n",
    "        'successes': results.get('successes'),\n",
    "        'success_rate': results.get('success_rate'),\n",
    "        'mean_reward': results.get('mean_reward'),\n",
    "    }]))\n",
    "    plot_eval_results(results, FIG_DIR)\n",
    "    for name in ['eval_rewards.png', 'eval_rewards_hist.png']:\n",
    "        img_path = FIG_DIR / name\n",
    "        if img_path.exists():\n",
    "            display(Image(filename=str(img_path)))\n",
    "else:\n",
    "    print(f'No se encontro el archivo de evaluacion: {EVAL_PATH}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}